#!/usr/bin/env python3

#~ import argparse
import click
import datetime
import infer
import neural_network
import os
import os.path
from read_quality import read_quality
import shutil
import split_f5
import subprocess
from sys import argv

"""
Main script to run tool from 
"""

# argparse
#~ parser = argparse.ArgumentParser()
#~ parser.add_argument("input", help="path to input file in FAST5 format")
#~ args = parser.parse_args()

# 1. Get input from user:
#       * fast5 file
#       * output file name
#       * specify some options?
#       * specify some output?

# 2. Infer signal

# 3. Save to file

# 4. Give output message

# TODO: change in infer length of raw!!
@click.command()
@click.option("--input-dir", "-i", help="Path to input dir in FAST5 format")
@click.option("--output-file", "-o", help="Name of read quality output file")
@click.option("--save-dir", "-s", help="Directory to save basecalled reads to")
def main(input_dir, output_file, save_dir, threshold=0.5, network_path="/mnt/scratch/thijs030/actualnetworks/ResNet-RNN_33", network_type="ResNetRNN"):
    """
    A tool with a neural network as basis to predict the presence of 
    homopolymers in the raw signal from a MinION sequencer. 
    """
    # 0. Load model
    t1 = datetime.datetime.now()
    model = neural_network.load_network(network_type, network_path, checkpoint=30000)
    print("Loaded model in {}".format(datetime.datetime.now() - t1))
    
    # 1. Load files in directory
    input_dir = os.path.abspath(input_dir)
    input_files = os.listdir(input_dir)

    # ONLY for HPs right now ##################################################
    # 2. Get predicted HPs for every read
    print("Checking for homopolymers in raw signal..")
    t2 = datetime.datetime.now()
    all_reads = [[fast5_file, infer.infer_class_from_signal("{}/{}".format(input_dir, fast5_file), model, label=1)] 
                for fast5_file in input_files]
    print("Finished classifications in {}".format(datetime.datetime.now() - t2))
    
    # save HP regions to dict
    hp_dict = {all_reads[ar][0] : all_reads[ar][1] for ar in range(len(all_reads))
                if all_reads[ar][1] != []}
    print("In {} out of {} reads at least one HP is detected in total of {}".format(len(hp_dict), len(input_files), datetime.datetime.now() - t2)) # remove later
    
    #~ print(hp_dict)
    
    # 3. Split FAST5 in multiple FAST5s
        # a. Save which files belong to group HP and to group non-HP
    temp_dir = "{}/TEMP".format(os.path.abspath(input_dir))
    temp_dir_hp = "{}/HP".format(os.path.abspath(temp_dir))
    temp_dir_nonhp = "{}/nonHP".format(os.path.abspath(temp_dir))
    os.makedirs(temp_dir_hp)
    #~ os.mkdir(temp_dir_nonhp)
    files_hp = []
    files_nonhp = []        # for non-HPs: get positions inbetween splits
    
    print("Splitting reads...")
    t3 = datetime.datetime.now()
    for read in hp_dict:
        #~ for i in range(len(hp_dict[read])):
            #~ if hp_dict[read][i][0] == 0:
        hp_files, nonhp_files = split_f5.split_signal("{}/{}".format(input_dir, read), hp_dict[read], temp_dir_hp)
        files_hp.extend(hp_files)                                               # HP files are absolute paths
        files_nonhp.extend(nonhp_files)
    print("Finished splitting the signal in {}".format(datetime.datetime.now() - t3))
    
    print("Number of files containing HPs after split: ", len(files_hp))
    print("Number of files containing non-HPs after split: ", len(files_nonhp))
    ## TODO: change later to not return anything. Is not necessary.

    # 4. Process each FAST5 belong to HP or non-HP group
    print("Basecalling with Albacore v2.3.3  ..")
    save_dir_hp = save_dir + "_HP"
    save_dir_nonhp = save_dir + "_nonHP"
    THREADS = 4
    CONFIG = "/mnt/nexenta/thijs030/data/r94_450bps_linear.cfg"                 # TODO: adjust!
    FLOW = "FLO-MIN106"
    KIT = "SQK-RAD002"            
    OUTPUT = "fastq"                                                            # can be FAST5 or FASTQ or both ; this for testing - maybe make variable?
    subprocess.run("source activate basecall", shell=True)
        # a. Register number of failed and number of passes (in total and per group)
    subprocess.run("read_fast5_basecaller.py -i {} -t {} -s {} -o {} -r -f {} -k {}".format(
                    temp_dir_hp, THREADS, save_dir_hp, OUTPUT, FLOW, KIT), shell=True)
    # close conda env
    subprocess.run("source deactivate", shell=True) 
           
        # b. Remove split files
    shutil.rmtree(temp_dir, ignore_errors=True)
    print("Removed TEMP directory.") # works :)
        
    # 5. Extract FASTAs from FASTQs in passed dir / Concatenate reads
        # a. Solve problem that multiple reads can be in one file - Is this a problem or should reads be longer?
        # cat ~/workdir/1D_basecall/workspace/pass/*.fastq > ~/workdir/1D_basecall.fastq
        
        # maybe use fast5seek
        # does albacore use duration? (or start time?) because I just copy this to all subfiles
    
    
    # 6. Check read quality
    REF_FASTA = "/mnt/nexenta/thijs030/data/reference/ecoli_ref.fasta"          # TODO: change
    read_quality(reads, REF_FASTA, output_file)      # reads can be folder of FASTQs or FASTAs - but first concat per read
    
    # 7. MAYBE - produce nice graphs on read quality and CPU speed
    
    
    #~ # basecall all reads with Albacore:



    
    # skip failed reads from Albacore
    
    # polish reads with HPs:
    
        
        # IDEE: code zo aanpassen dat basecaller het meteen inneemt?
            
            # let non-HP regions go through fast basecaller
            
            # let (extended) HP reads go through polisher / specialized basecaller
            
            # write basecalls to FAST5
            
            # if something goes wrong:
            #~ except Error as e:
    # write unsuccessfull reads to file
    #~ with open(output_file, "w") as dest:                                        # output_file is error file
        #~ dest.write("Unable to basecall the following reads: \n\n")            
        #~ failed_reads = []
        #~ dest.write("{}: {}\n".format(e, read_name))            
        
    #~ print("Successfully basecalled {} of {} reads.".format(successes, len()) # is not necessary: albacore splits this up already
    
    #~ return None

    
if __name__ == "__main__":
    main()
    




